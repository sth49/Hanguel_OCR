{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce159a66",
   "metadata": {},
   "source": [
    "# 0. 라이브러리 및 개발 환경\n",
    "\n",
    "### 0-1) 라이브러리 - 함께 제출한 ocr_env.yaml 파일 참고\n",
    "### 0-2) 개발 환경 - Ubuntu 20.04.4 LTS \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d1826",
   "metadata": {},
   "source": [
    "# 1. 데이터셋 관련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183faa74",
   "metadata": {},
   "source": [
    "대회에서 제공한 데이터: train, test, train.csv, test.csv, sample_submission.csv\n",
    "\n",
    "외부 데이터: Aihub 야외 실제 촬영 한글 이미지 \n",
    "https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=105\n",
    "\n",
    "[train]  \n",
    "\n",
    "- [라벨]Training.zip\n",
    "- [원천]Training_간판_실내간판_원천데이터1.zip\n",
    "- [원천]Training_간판_돌출간판_원천데이터1.zip  \n",
    "- [원천]Training_간판_세로형간판_원천데이터.zip\n",
    "- [원천]Training_간판_실내안내판_원천데이터1.zip\n",
    "- [원천]Training_간판_가로형간판_원천데이터1.zip\n",
    "\n",
    "[validation]  \n",
    "\n",
    "- [라벨]Validation.zip\n",
    "- [원천]Validation_간판3.zip\n",
    "- [원천]Validation_간판1.zip\n",
    "- [원천]Validation_간판2.zip\n",
    "\n",
    "### 1-1) dataset 폴더를 생성하고, 대회에서 제공한 데이터와 외부 데이터들을 모두 넣어준다. \n",
    "\n",
    "\n",
    "```bash\n",
    "├── dataset\n",
    "         ├── train\n",
    "         ├── test\n",
    "         ├── train.csv\n",
    "         ├── test.csv\n",
    "         ├── sample_submission.csv\n",
    "         ├── [라벨]Training.zip\n",
    "         ├── [원천]Training_간판_실내간판_원천데이터1.zip\n",
    "         ├── [원천]Training_간판_돌출간판_원천데이터1.zip  \n",
    "            .\n",
    "            .\n",
    "            .\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09092",
   "metadata": {},
   "source": [
    "### 1-2) 압축 풀고, 폴더명 변경\n",
    "\n",
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c941ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./dataset/[라벨]Training.zip -d ./dataset/label/\n",
    "!unzip ./dataset/[원천]Training_간판_가로형간판_원천데이터1.zip -d ./dataset/raw_train2/\n",
    "!unzip ./dataset/[원천]Training_간판_돌출간판_원천데이터1.zip -d ./dataset/raw_train3/\n",
    "!unzip ./dataset/[원천]Training_간판_세로형간판_원천데이터.zip -d ./dataset/raw_train4/\n",
    "!unzip ./dataset/[원천]Training_간판_실내간판_원천데이터1.zip -d ./dataset/raw_train5/\n",
    "!unzip ./dataset/[원천]Training_간판_실내안내판_원천데이터1.zip -d ./dataset/raw_train6/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "50b0a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./dataset/label/1.간판/1.가로형간판/가로형간판1/ ./dataset/label_train2/\n",
    "!mv ./dataset/label/1.간판/2.돌출간판/돌출간판1/ ./dataset/label_train3/\n",
    "!mv ./dataset/label/1.간판/3.세로형간판/ ./dataset/label_train4/\n",
    "!mv ./dataset/label/1.간판/4.실내간판/실내간판1/ ./dataset/label_train5/\n",
    "!mv ./dataset/label/1.간판/5.실내안내판/새\\ 폴더/ ./dataset/label_train6/\n",
    "!rm -rf ./dataset/label/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142c271",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23887c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./dataset/[라벨]Validation.zip -d ./dataset/label_val/\n",
    "!unzip ./dataset/[원천]Validation_간판1.zip -d ./dataset/raw_val/\n",
    "!unzip ./dataset/[원천]Validation_간판2.zip -d ./dataset/raw_val/\n",
    "!unzip ./dataset/[원천]Validation_간판3.zip -d ./dataset/raw_val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607c9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./dataset/raw_val/01.가로형간판/ ./dataset/raw_val1/\n",
    "!mv ./dataset/raw_val/02.돌출간판/ ./dataset/raw_val2/\n",
    "!mv ./dataset/raw_val/03.세로형간판/ ./dataset/raw_val3/\n",
    "!mv ./dataset/raw_val/04.실내간판/ ./dataset/raw_val4/\n",
    "!mv ./dataset/raw_val/05.실내안내판/ ./dataset/raw_val5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd386e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./dataset/label_val/1.간판/1.가로형간판/ ./dataset/label_val1/\n",
    "!mv ./dataset/label_val/1.간판/2.돌출간판/ ./dataset/label_val2/\n",
    "!mv ./dataset/label_val/1.간판/3.세로형간판/ ./dataset/label_val3/\n",
    "!mv ./dataset/label_val/1.간판/4.실내간판/ ./dataset/label_val4/\n",
    "!mv ./dataset/label_val/1.간판/5.실내안내판/ ./dataset/label_val5/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ee8ad",
   "metadata": {},
   "source": [
    "### 1-3) 외부 데이터를 주어진 annotation file 정보를 사용하여 cropping  후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "01ad3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unicodedata import normalize\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def raw_cropping(img_path, label_path, save_path, tp):\n",
    "    imgdir = os.listdir(img_path)\n",
    "    label=[]\n",
    "    print(len(imgdir))\n",
    "    \n",
    "    for i, img in enumerate(imgdir):\n",
    "        if tp=='train4' and(i==1169 or i==5253):\n",
    "            continue\n",
    "#         print(i)\n",
    "        nfc_file = normalize(\"NFC\", os.path.join(img_path, img))\n",
    "        src = cv2.imread(nfc_file, cv2.IMREAD_COLOR)\n",
    "        try:\n",
    "            img_word = img.split('.')\n",
    "            with open(os.path.join(label_path,img_word[0]+'.json'), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        ant = data['annotations']\n",
    "        for j, a in enumerate(ant):\n",
    "            text = a['text']\n",
    "            bbox = a['bbox']\n",
    "            if text==('xxx'):\n",
    "                continue\n",
    "            try:\n",
    "                if bbox[0]<0:\n",
    "                    bbox[0]=0\n",
    "                elif bbox[1]<0:\n",
    "                    bbox[1]=0\n",
    "                elif bbox[2]<0 or bbox[3]<0:\n",
    "                    continue\n",
    "                for k, b in enumerate(bbox):\n",
    "                    if b<0:\n",
    "                        bbox[k]=0\n",
    "                name = './'+tp+'/'+tp+'_'+str(i).zfill(5)+'_'+str(j).zfill(2)+'.png'\n",
    "                dst=src.copy()\n",
    "                label.append([name,text])\n",
    "                dst=dst[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
    "                save = os.path.join(save_path, tp+'_'+str(i).zfill(5)+'_'+str(j).zfill(2)+'.png')\n",
    "                cv2.imwrite(save, dst)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    df = pd.DataFrame(label,columns=['img_path','text'])\n",
    "    df.to_csv(os.path.join('./dataset/',tp+'.csv'), index=None, encoding='utf-8-sig')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaa005",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d87faebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25549\n"
     ]
    }
   ],
   "source": [
    "tps = ['train2', 'train2','train3', 'train4', 'train5','train6']\n",
    "for tp in tps:\n",
    "    path = './dataset/'\n",
    "    img_path = os.path.join(path, f'raw_{tp}')\n",
    "    label_path = os.path.join(path, f'label_{tp}')\n",
    "    save_path = os.path.join(path, f'{tp}')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    raw_cropping(img_path, label_path, save_path, tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ab487",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5900192",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps = ['val1', 'val2','val3', 'val4', 'val5']\n",
    "for tp in tps:\n",
    "    path = './dataset/'\n",
    "    img_path = os.path.join(path, f'raw_{tp}')\n",
    "    label_path = os.path.join(path, f'label_{tp}')\n",
    "    save_path = os.path.join(path, f'{tp}')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    raw_cropping(img_path, label_path, save_path, tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e7355",
   "metadata": {},
   "source": [
    "### 1-4) vert / hori 폴더 나누고 gt 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1702105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(tp):\n",
    "    hori = []\n",
    "    vert = []\n",
    "    df_train = pd.read_csv(f'./dataset/{tp}.csv')\n",
    "    texts = df_train['text'].tolist()\n",
    "    \n",
    "    img_path = f'./dataset/{tp}/'\n",
    "    imgs = sorted(os.listdir(img_path))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        img = cv2.imread(os.path.join(img_path,imgs[i]))\n",
    "        h, w, _ = img.shape\n",
    "        if w>h:\n",
    "            cv2.imwrite(os.path.join(path,  f'train_hori/'+imgs[i]), img)\n",
    "            imagepath = os.path.join(\"./train_hori/\",imgs[i])\n",
    "            hori.append(f\"{imagepath}\\t{text}\\n\")\n",
    "        else:\n",
    "            cv2.imwrite(os.path.join(path, f'train_vert/'+imgs[i]), img)\n",
    "            imagepath = os.path.join(\"./train_vert/\",imgs[i])\n",
    "            vert.append(f\"{imagepath}\\t{text}\\n\")\n",
    "\n",
    "    return hori, vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9ae4469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hori =[]\n",
    "all_vert = []\n",
    "path = './dataset/'\n",
    "os.makedirs(os.path.join(path, f'train_hori/'), exist_ok=True)\n",
    "os.makedirs(os.path.join(path, f'train_vert/'), exist_ok=True)\n",
    "tps = ['train', 'train2', 'train3', 'train4', 'train5', 'train6']\n",
    "for tp in tps: \n",
    "    temp_hori, temp_vert = split_train(tp)\n",
    "    all_hori += temp_hori\n",
    "    all_vert += temp_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "81ef62ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470\n",
      "65436\n"
     ]
    }
   ],
   "source": [
    "file = open(\"./dataset/gt_train_hori.txt\",\"w\")\n",
    "for line in all_hori:\n",
    "        file.write(line)\n",
    "file.close()\n",
    "print(len(all_hori))\n",
    "\n",
    "file = open(\"./dataset/gt_train_vert.txt\",\"w\")\n",
    "for line in all_vert:\n",
    "        file.write(line)\n",
    "file.close()\n",
    "print(len(all_vert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f41e28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_val(tp):\n",
    "    hori = []\n",
    "    vert = []\n",
    "    df_val = pd.read_csv(f'./dataset/{tp}.csv')\n",
    "    texts = df_val['text'].tolist()\n",
    "    \n",
    "    img_path = f'./dataset/{tp}/'\n",
    "    imgs = sorted(os.listdir(img_path))\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        img = cv2.imread(os.path.join(img_path,imgs[i]))\n",
    "        h, w, _ = img.shape\n",
    "        if w>h:\n",
    "            cv2.imwrite(os.path.join(path,  f'val_hori/'+imgs[i]), img)\n",
    "            imagepath = os.path.join(\"./val_hori/\",imgs[i])\n",
    "            hori.append(f\"{imagepath}\\t{text}\\n\")\n",
    "        else:\n",
    "            cv2.imwrite(os.path.join(path, f'val_vert/'+imgs[i]), img)\n",
    "            imagepath = os.path.join(\"./val_vert/\",imgs[i])\n",
    "            vert.append(f\"{imagepath}\\t{text}\\n\")\n",
    "\n",
    "    return hori, vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "70b53e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hori_val =[]\n",
    "all_vert_vert = []\n",
    "path = './dataset/'\n",
    "os.makedirs(os.path.join(path, f'val_hori/'), exist_ok=True)\n",
    "os.makedirs(os.path.join(path, f'val_vert/'), exist_ok=True)\n",
    "tps = ['val1', 'val2', 'val3', 'val4', 'val5']\n",
    "for tp in tps: \n",
    "    temp_hori, temp_vert = split_val(tp)\n",
    "    all_hori_val += temp_hori\n",
    "    all_vert_vert += temp_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9f388410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53546\n",
      "12941\n"
     ]
    }
   ],
   "source": [
    "file = open(\"./dataset/gt_val_hori.txt\",\"w\")\n",
    "for line in all_hori_val:\n",
    "        file.write(line)\n",
    "file.close()\n",
    "print(len(all_hori_val))\n",
    "\n",
    "file = open(\"./dataset/gt_val_vert.txt\",\"w\")\n",
    "for line in all_vert_vert:\n",
    "        file.write(line)\n",
    "file.close()\n",
    "print(len(all_vert_vert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8723daa",
   "metadata": {},
   "source": [
    "### 1-5) 최종 dataset 폴더\n",
    "\n",
    "\n",
    "```bash\n",
    "├── dataset\n",
    "         ├── train_hori\n",
    "         ├── train_vert\n",
    "         ├── gt_train_hori.txt\n",
    "         ├── gt_train_vert.txt\n",
    "         ├── val_hori\n",
    "         ├── val_vert\n",
    "         ├── gt_val_vert.txt\n",
    "         ├── gt_val_vert.txt\n",
    "         ├── test\n",
    "         ├── sample_submission.csv\n",
    "\n",
    "``` \n",
    "\n",
    "이외의 파일/폴더 지우기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc3d42",
   "metadata": {},
   "source": [
    "# 2. 학습 관련\n",
    "\n",
    "### 2-1) deep-text-recognition-benchmark 다운로드 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/clovaai/deep-text-recognition-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb41cdb",
   "metadata": {},
   "source": [
    "```bash\n",
    "├── dataset\n",
    "│       ├── train_hori\n",
    "│       ├── train_vert\n",
    "│       ├── gt_train_hori.txt\n",
    "│       ├── gt_train_vert.txt\n",
    "│       ├── val_hori\n",
    "│       ├── val_vert\n",
    "│       ├── gt_val_vert.txt\n",
    "│       ├── gt_val_vert.txt\n",
    "│       ├── test\n",
    "│       ├── sample_submission.csv\n",
    "├── deep-text-recognition-benchmark\n",
    "\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973f658",
   "metadata": {},
   "source": [
    "### 2-2) 학습을 위해 전처리한 데이터를 lmdb로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358ae9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath ./dataset/ --gtFile ./dataset/gt_train_hori.txt --outputPath ./deep-text-recognition-benchmark/data_lmdb/train_hori/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34abe2a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath ./dataset/ --gtFile ./dataset/gt_train_vert.txt --outputPath ./deep-text-recognition-benchmark/data_lmdb/train_vert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath ./dataset/ --gtFile ./dataset/gt_val_vert.txt --outputPath ./deep-text-recognition-benchmark/data_lmdb/val_vert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8052fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 ./deep-text-recognition-benchmark/create_lmdb_dataset.py --inputPath ./dataset/ --gtFile ./dataset/gt_val_hori.txt --outputPath ./deep-text-recognition-benchmark/data_lmdb/val_hori/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5ac08",
   "metadata": {},
   "source": [
    "### 2-3) train 코드 수정\n",
    "\n",
    "```python\n",
    "# 추가사항\n",
    "parser.add_argument('--tp', default='vert', help='the type of model')\n",
    "\n",
    "\n",
    "# 수정사항\n",
    "parser.add_argument('--character', type=str,\n",
    "                        default='골 목미용실한성부동산홍라운지모단걸응접씨앗양식하정소의원화가구백점마훠궈오로클래츠반상저온숙수제전문치과유황리참숯불우림자작나무금강포장중기조헤어샵까끄아영빈케익스활선약국밀돼밥세신피노키경이진철거주필립터디카페빙그레트바머연출윤네멸쿡즈데예건축인월천공물꽃니쿨방설집옥최고등급드곤보울야토탈녀복렌창업심콜렉션테내몸에손탁텔린취는음악학른프임호희망개사찬명통증교크승족핸맨닥옷만혜빌민패꼬김당티타감각더벽글벌여행쌤님돈퍼관탐습코푸도차현계일채폰센애플서쉼생술쿠르따뜻송커힐대쉐놀갈비객즐다킨믹루큰워뷰직력곰맥할맛있회올안환베남쌀랑콩뜰달북먹발틈새극재록란쭈꾸찌판매능샘블속독법분롱짐본엄냉닭합염군탑광열엔류튜살헨느빠풍시요게랜엘틸종샷와박규형외딩흉충두파벨또름배청친효돌쌈길틱을앤쓰담잔위쏠힘쎄젤싸빵료은퀼컷꼴투콤향핀초십삼짇날씽련택해석탄팽떼쎼닛딸링젠캔버봄샤뮤엠메칼탕슉번째샐침태후랄늘러브휴냥표볶넬쁘곱쫄댄릇빗쥬얼육체혼좋으뜸누떡낭겹닉품든옛팬밤삭잉면즉끓총별빛춘셀큐흥막럽곡준턴샹릴귀델팔득슈끼평싶컬캐롬눔욕빨꺼책핑줄밴순햇앙촌너허웰핏낙쇄밍결헬훈탭굽홈권닝뉴템꿈켈람층릉알휘항간곽근풀편콘텐역핌룩팥봉룸붐찜퓨덕컴완펜댕숭톰엌럴깔끔샬협쿄함입뎅쩐둥솔횟왕율쌍멋칠앵때릭견특깃짬뽕갤난례멀획컨팅량멍텅릿쇼룹샌닮언왁싱눈썹꾼추털뜨락쉬숍쵸팡듬짝맵랙잠갑께뽀흑색흠둑쇠뇌혈암쏘펠빼찻멘들히롯년끌켓꿀뻘잇옆착븐룡쎈깨슬홀믿몽쏭핫펌볼륨캡굿짱끝뽈쑝뚝딱짜엣병숲널몰써덴던뭄죽궁낵긴튬웃뭉텀겐펍댁쭌칭멜벤처땅랩붙검킴솥엉범윈쟁낚샨똘킹격렁된롤적뇽썰잡를퀸몬넷존넘액윌뇨셉농펫팰며튀압뢰웨덤쉽받싼런팩욱밸밭져썸맹뱃쁨윗옴므령돔맘틀럭륜말셋뷔논읽닐험융려떤척닷밝텍턱룻짚넌첨씬절풋묘곳납왓옵픽퀘캠질렐톡젼헛넥흰뱅쁠띠깐뎃탠밌칸빅톤팝률옻벧확듀덮퉁섬딘즌튼넝꼼캣론럼쉴측젝쿱흡맑잼팀엑렛웹죠딕곁잎좌컵퀵떳쾌높딧벡놔쫌얄렙윙쳐변뎀즘랭없렬넣흔톨녹맞춤굴및움뚱잭솟밧픈찰슨롭삽녕붕슐쿤쟈맴뽁켐솜팜긋숨젬억졸셨첫뒤될셜찹빔됩챌꽁먼뚜펙킥뒷깜빡헌폐캬뽑삘툴겨땡떙럿겁렘냄곧셔흐균힌즙첸많념쩡칡징딴햄뼈웅춧퇴벚킬뻐썬츄촬옳껍벗앰묵찾뵙겠랫폼샾램렵숴봐갓넓떠앞뚫닙켄되싯왔씻잘섯략않것꼰혹긍답짧폴켑랐칙핵폭혁왜묻냐엇떻괴했겅짓삶겔덩훙촛듯쓴옮멧큘싫씩괜찮죄혐걱낳았뭐둠퀴쓸끈었렇겼꿔멈좁뭔딜잃봤앓팍햬헥귝얀듦같씀뿌엽훔걷깊휼엿냈쁜놓밖큼낫닌맙봇닫둔뀐캇듣붓랍켜숫잊혀틴튤못벼떨긷뛰슴찢맺줍쪽붉첩늙귓괘갔헝섹쥐낮챔캉섭쯤좀줘졌딥캘옙걀둘덟씹헸웠볍였픔셰잖낸핍냠넙뻔콕엮얻솝챙랬펀뮈셸쉿뻬흘꽂칵몇툰깥꼭낯탉놈빚늑줌첼삿뮌찐햅펭귄빽옹댓맷퀀잌끗꽈넛숩숀슥쉘룽싹륭훼왼흙똥갱겟콧뮬헐룰믈푼샛벳쑈퐁읍쿼벅앨뫼뗌롶껑웬삐톱값딤쁄훌젓펄줏쏙짙텝볕궐렴펑뉘탱륙꿉꾹얌떴쿰늄밑숑킷돗팻귤뀨욜뤼셴똑꺽웜꿍쿵돝깡쥴밋겸뱍띵쪼쌩갯젊텃헉랏굼랴똔깻긱팟휠덱찍멤깍븟멕쑥뺑쟝궃촉샴쏨녘갸윰픙뱀떄벙늪뿔뿍껏숖홉듸숟븍셈썅묭밈쯔맟캄낌탤쓱릎섞찡굯쏜솣쫀꼽굉돋묶씰힙쥰삥챠땐껴햐탬쭉횡촨뽄옾땀곶굄멩껌챈꾀켠룬잣텬웁깅윔낼쑤툼륵둣얏녜돛풉챨쭝엥깎욘뙈갭녁쪄훗웍덜뺴듭튠쮸얹쉔뻥괸씸쏀왈쨈섀옐갖뢍괄툇짠댑헹캅뀀껄휀똣넋빳틔댐띄헵꽐썽쳇눌쎌떢렷롸넉힝뿐얜쑨챤뵈볏묾쩌깽쟌뿜랠칫죤뺀캥댥퀄렝뭇챕칩뺭뽐맬샥냇퇘왠켘쌉횃댜섶몀큽펴얇슌슁끊엎슘졍돐숄넵톳팸젯빻붎꼐셩읜햔꿩댈딪늬빤잽깝볽됨쏴뛴컹땍틉끽꺠앉뤠릅쯴늠웸짤뻑뼝쫑쩍갬뗑앱뜌뜜쩜쌔섧잗뗴넨쨍껀릐눕뷜낄푹슟뭘꼈낀뚤뜬뜯삑냅뀝횰쩨쎔쌓놋샆췬힉싀헙앳껨덧윷겜놉싣뎁캌윅팁펏났옌펼핯훨썩뫔궂잿촘췄뚠땁얘튈깁봅짭벵킵넴빱엡쇳쉰딍띤삔갠콥쿳싁럷툽틍젖', help='character label')\n",
    "\n",
    "parser.add_argument('--select_data', type=str, default='/',\n",
    "                        help='select training data (default is MJ-ST, which means MJ and ST used as training data)')\n",
    "\n",
    "parser.add_argument('--batch_ratio', type=str, default='1',\n",
    "                    help='assign ratio for each selected data in the batch')\n",
    "\n",
    "if not opt.exp_name:\n",
    "        opt.exp_name = f'{opt.tp}-{opt.Transformation}-{opt.FeatureExtraction}-{opt.SequenceModeling}-{opt.Prediction}'\n",
    "        opt.exp_name += f'-Seed{opt.manualSeed}'\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33872e34",
   "metadata": {},
   "source": [
    "### 2-4) 학습 진행\n",
    "\n",
    "Cuda out of memory가 뜰 경우, 배치사이즈를 줄여 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./deep-text-recognition-benchmark/train.py --train_data ./deep-text-recognition-benchmark/data_lmdb/train_vert/ --valid_data ./deep-text-recognition-benchmark/data_lmdb/val_vert/ --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction CTC --data_filtering_off --batch_size 512 --batch_max_length 200 --workers 4 --num_iter 100000 --valInterval 100 --imgH 200 --imgW 64 --tp vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python ./deep-text-recognition-benchmark/train.py --train_data ./deep-text-recognition-benchmark/data_lmdb/train_hori/ --valid_data ./deep-text-recognition-benchmark/data_lmdb/val_hori/ --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction CTC --data_filtering_off --batch_size 256 --batch_max_length 200 --workers 4 --num_iter 100000 --valInterval 100 --imgH 64 --imgW 200 --tp hori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f827507",
   "metadata": {},
   "source": [
    "# 3. 제출파일 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f969e0",
   "metadata": {},
   "source": [
    "### 3-1) 가로, 세로 모델에 대한 추론 파일 생성\n",
    "```python\n",
    "import string\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import CTCLabelConverter, AttnLabelConverter\n",
    "from dataset import RawDataset, AlignCollate\n",
    "from model import Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def demo(opt):\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "    # model = torch.nn.DataParallel(model).to(device)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # load model\n",
    "    print('loading pretrained model from %s' % opt.saved_model)\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "\n",
    "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "    save_data = []\n",
    "    # predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text_for_pred)\n",
    "\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                # preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode(preds_index, preds_size)\n",
    "\n",
    "            else:\n",
    "                preds = model(image, text_for_pred, is_train=False)\n",
    "\n",
    "                # select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "\n",
    "            log = open(f'./log_demo_result.txt', 'a')\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
    "            \n",
    "            # print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "            log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            \n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "                # calculate confidence score (= multiply of pred_max_prob)\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "\n",
    "                # print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
    "                img_name = './'+img_name\n",
    "                save_data.append([img_name, pred])\n",
    "            df = pd.DataFrame(save_data, columns=['img_path', 'text'])\n",
    "            df.to_csv(f'./{opt.tp}_result.csv', index=None, encoding='utf-8-sig')\n",
    "            log.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_folder', required=True, help='path to image_folder which contains text images')\n",
    "    parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='input batch size')\n",
    "    parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
    "    \"\"\" Data processing \"\"\"\n",
    "    parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
    "    parser.add_argument('--imgH', type=int, default=64, help='the height of the input image')\n",
    "    parser.add_argument('--imgW', type=int, default=200, help='the width of the input image')\n",
    "    parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
    "    parser.add_argument('--character', type=str,\n",
    "                        default='골 목미용실한성부동산홍라운지모단걸응접씨앗양식하정소의원화가구백점마훠궈오로클래츠반상저온숙수제전문치과유황리참숯불우림자작나무금강포장중기조헤어샵까끄아영빈케익스활선약국밀돼밥세신피노키경이진철거주필립터디카페빙그레트바머연출윤네멸쿡즈데예건축인월천공물꽃니쿨방설집옥최고등급드곤보울야토탈녀복렌창업심콜렉션테내몸에손탁텔린취는음악학른프임호희망개사찬명통증교크승족핸맨닥옷만혜빌민패꼬김당티타감각더벽글벌여행쌤님돈퍼관탐습코푸도차현계일채폰센애플서쉼생술쿠르따뜻송커힐대쉐놀갈비객즐다킨믹루큰워뷰직력곰맥할맛있회올안환베남쌀랑콩뜰달북먹발틈새극재록란쭈꾸찌판매능샘블속독법분롱짐본엄냉닭합염군탑광열엔류튜살헨느빠풍시요게랜엘틸종샷와박규형외딩흉충두파벨또름배청친효돌쌈길틱을앤쓰담잔위쏠힘쎄젤싸빵료은퀼컷꼴투콤향핀초십삼짇날씽련택해석탄팽떼쎼닛딸링젠캔버봄샤뮤엠메칼탕슉번째샐침태후랄늘러브휴냥표볶넬쁘곱쫄댄릇빗쥬얼육체혼좋으뜸누떡낭겹닉품든옛팬밤삭잉면즉끓총별빛춘셀큐흥막럽곡준턴샹릴귀델팔득슈끼평싶컬캐롬눔욕빨꺼책핑줄밴순햇앙촌너허웰핏낙쇄밍결헬훈탭굽홈권닝뉴템꿈켈람층릉알휘항간곽근풀편콘텐역핌룩팥봉룸붐찜퓨덕컴완펜댕숭톰엌럴깔끔샬협쿄함입뎅쩐둥솔횟왕율쌍멋칠앵때릭견특깃짬뽕갤난례멀획컨팅량멍텅릿쇼룹샌닮언왁싱눈썹꾼추털뜨락쉬숍쵸팡듬짝맵랙잠갑께뽀흑색흠둑쇠뇌혈암쏘펠빼찻멘들히롯년끌켓꿀뻘잇옆착븐룡쎈깨슬홀믿몽쏭핫펌볼륨캡굿짱끝뽈쑝뚝딱짜엣병숲널몰써덴던뭄죽궁낵긴튬웃뭉텀겐펍댁쭌칭멜벤처땅랩붙검킴솥엉범윈쟁낚샨똘킹격렁된롤적뇽썰잡를퀸몬넷존넘액윌뇨셉농펫팰며튀압뢰웨덤쉽받싼런팩욱밸밭져썸맹뱃쁨윗옴므령돔맘틀럭륜말셋뷔논읽닐험융려떤척닷밝텍턱룻짚넌첨씬절풋묘곳납왓옵픽퀘캠질렐톡젼헛넥흰뱅쁠띠깐뎃탠밌칸빅톤팝률옻벧확듀덮퉁섬딘즌튼넝꼼캣론럼쉴측젝쿱흡맑잼팀엑렛웹죠딕곁잎좌컵퀵떳쾌높딧벡놔쫌얄렙윙쳐변뎀즘랭없렬넣흔톨녹맞춤굴및움뚱잭솟밧픈찰슨롭삽녕붕슐쿤쟈맴뽁켐솜팜긋숨젬억졸셨첫뒤될셜찹빔됩챌꽁먼뚜펙킥뒷깜빡헌폐캬뽑삘툴겨땡떙럿겁렘냄곧셔흐균힌즙첸많념쩡칡징딴햄뼈웅춧퇴벚킬뻐썬츄촬옳껍벗앰묵찾뵙겠랫폼샾램렵숴봐갓넓떠앞뚫닙켄되싯왔씻잘섯략않것꼰혹긍답짧폴켑랐칙핵폭혁왜묻냐엇떻괴했겅짓삶겔덩훙촛듯쓴옮멧큘싫씩괜찮죄혐걱낳았뭐둠퀴쓸끈었렇겼꿔멈좁뭔딜잃봤앓팍햬헥귝얀듦같씀뿌엽훔걷깊휼엿냈쁜놓밖큼낫닌맙봇닫둔뀐캇듣붓랍켜숫잊혀틴튤못벼떨긷뛰슴찢맺줍쪽붉첩늙귓괘갔헝섹쥐낮챔캉섭쯤좀줘졌딥캘옙걀둘덟씹헸웠볍였픔셰잖낸핍냠넙뻔콕엮얻솝챙랬펀뮈셸쉿뻬흘꽂칵몇툰깥꼭낯탉놈빚늑줌첼삿뮌찐햅펭귄빽옹댓맷퀀잌끗꽈넛숩숀슥쉘룽싹륭훼왼흙똥갱겟콧뮬헐룰믈푼샛벳쑈퐁읍쿼벅앨뫼뗌롶껑웬삐톱값딤쁄훌젓펄줏쏙짙텝볕궐렴펑뉘탱륙꿉꾹얌떴쿰늄밑숑킷돗팻귤뀨욜뤼셴똑꺽웜꿍쿵돝깡쥴밋겸뱍띵쪼쌩갯젊텃헉랏굼랴똔깻긱팟휠덱찍멤깍븟멕쑥뺑쟝궃촉샴쏨녘갸윰픙뱀떄벙늪뿔뿍껏숖홉듸숟븍셈썅묭밈쯔맟캄낌탤쓱릎섞찡굯쏜솣쫀꼽굉돋묶씰힙쥰삥챠땐껴햐탬쭉횡촨뽄옾땀곶굄멩껌챈꾀켠룬잣텬웁깅윔낼쑤툼륵둣얏녜돛풉챨쭝엥깎욘뙈갭녁쪄훗웍덜뺴듭튠쮸얹쉔뻥괸씸쏀왈쨈섀옐갖뢍괄툇짠댑헹캅뀀껄휀똣넋빳틔댐띄헵꽐썽쳇눌쎌떢렷롸넉힝뿐얜쑨챤뵈볏묾쩌깽쟌뿜랠칫죤뺀캥댥퀄렝뭇챕칩뺭뽐맬샥냇퇘왠켘쌉횃댜섶몀큽펴얇슌슁끊엎슘졍돐숄넵톳팸젯빻붎꼐셩읜햔꿩댈딪늬빤잽깝볽됨쏴뛴컹땍틉끽꺠앉뤠릅쯴늠웸짤뻑뼝쫑쩍갬뗑앱뜌뜜쩜쌔섧잗뗴넨쨍껀릐눕뷜낄푹슟뭘꼈낀뚤뜬뜯삑냅뀝횰쩨쎔쌓놋샆췬힉싀헙앳껨덧윷겜놉싣뎁캌윅팁펏났옌펼핯훨썩뫔궂잿촘췄뚠땁얘튈깁봅짭벵킵넴빱엡쇳쉰딍띤삔갠콥쿳싁럷툽틍젖', help='character label')\n",
    "    parser.add_argument('--sensitive', action='store_true', help='for sensitive character mode')\n",
    "    parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
    "    \"\"\" Model Architecture \"\"\"\n",
    "    parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
    "    parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
    "    parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
    "    parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
    "    parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
    "    parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
    "    parser.add_argument('--output_channel', type=int, default=512,\n",
    "                        help='the number of output channel of Feature extractor')\n",
    "    parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n",
    "    parser.add_argument('--tp', type=str, default='vert', help='the size of the LSTM hidden state')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    \"\"\" vocab / character number configuration \"\"\"\n",
    "    if opt.sensitive:\n",
    "        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True\n",
    "    opt.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "    demo(opt)\n",
    "\n",
    "    \n",
    "```\n",
    "demo.py - 모델의 추론 결과를 csv로 저장하는 형태로 수정하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c8077",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./deep-text-recognition-benchmark/deom.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction CTC --image_folder ./dataset/test/ --saved_model ./saved_models/hori-TPS-ResNet-BiLSTM-CTC-Seed1111/best_accuracy.pth --imgH 64 --imgW 200 --tp hori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python ./deep-text-recognition-benchmark/deom.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction CTC --image_folder ./dataset/test/ --saved_model ./saved_models/vert-TPS-ResNet-BiLSTM-CTC-Seed1111/best_accuracy.pth --imgH 200 --imgW 64 --tp vert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272ea09",
   "metadata": {},
   "source": [
    "### 3-2) 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7c8d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "df_hori = pd.read_csv('hori_result.csv')\n",
    "df_vert = pd.read_csv('vert_result.csv')\n",
    "df_hori.set_index(\"img_path\", inplace=True)\n",
    "df_vert.set_index(\"img_path\", inplace=True)\n",
    "test_dir = './dataset/test/'\n",
    "imgs = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2fd6df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vert = []\n",
    "hori = []\n",
    "for img_name in imgs:\n",
    "    img = cv2.imread(os.path.join(test_dir,img_name))\n",
    "    if img is None:\n",
    "        continue\n",
    "    h, w, _ = img.shape\n",
    "    if w>h:\n",
    "        hori.append(img_name)\n",
    "    else:\n",
    "        vert.append(img_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "333c20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/sample_submission.csv')\n",
    "df.set_index(\"img_path\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ca7139d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    search = '././dataset'+i[1:]\n",
    "    if i[7:] in hori:\n",
    "        df.at[i, 'text']=df_hori.at[search,'text']\n",
    "    elif i[7:] in vert:\n",
    "        df.at[i, 'text']=df_vert.at[search,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "295b48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df.to_csv('./submit.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
